# Introducao ao Haddop
    Haddop:
    Uma estrutura de código aberto para bigdata, com os principais componentes: YARN, MapReduce, Hive, HDFS, Hadoop Commom    
        Como Hadoop trabalha:
        HDFS:
            Lida e armazena grande quantidade de dados
            Escala um unico cluster para milhares
        MapReduce:
            Conhecido como Haddop processing unit
            Processo dados de Big Data quebrando em dados menores
            Primeiro método usado para consultar dados armazenados no HDFS
        YARN:
            Yet Another Resource Negotiator
            Prepara a RAM e o CPU para o haddop executar os dados em batch, stream, interativo e gráfico

MapReduce:
    O que é:
        Um programa que permite escalabilidade massiva em centenas de milhares de servidores em um haddop cluster
        Técnicas de processamento para computaçao distribídas
        Consiste de uma tarefa de map e reduce
        Pode ser codado em Java,C ++, Pyrhon, Ruby e R
    Framework:
        Input File --> 
        Map; Executa tarefas de mapeamento extraindo informaçoes de dados -->
        Reduce; funciona com varias funções de map e agrega os conjuntos de dados a partir de uma mesma chave
    How MapReduce works:
    Why Map Reduce
        Computacao paralela

Haddop Ecosystem:
    Hbase:
        nao relacional databse
        permite manuseio do dado em tempo real
        armazena dado
    Cassandra:
        banco de dados NoSQL, escalável projetado ara nao ter um ponto unico de falha
    Pig:
        Analiza grande quantidade de dados
        Opera order do cliente
        Linguagem pro data flow que segue ordem e conjunto de comandaos
    Hive:
        Criado para gerar relatórios e opera no servidor de um cluster
        Linguagem de programação declarativa onde o usuario declara os dados e tipos que deseja receber
    Acces Data
    Impala:
        Escalavel e uma plataforma dácil de usar para todos
    Hue:
        Permite upar, baixar, browse e query de dados
        Permite executar Pig Apache jobs 
        Fornece um SQL para varias linguagens de consulta

HDFS:
    Sistema de arquivos distribuídos em vários servidores de arquivos, que permidere acessar em qualquer servidor
    Camada de armazenamento do Haddop
    Quebra os arquivos em blocos cria réplicas dos blocos e armazena em diferentes máquinas.
    Prove acesso a stream de dados sem problemas
Principais beneficios
    Armazenado de hardwate nao é caro; a camada de armazenamento não é cara
    grande quantidade de dados; pode armazenar petabytes de dados
    replicação; cópias dos dados em multiplas máquinas
    Tolerante a falha; se uma máquina quebra, uma cópia dos dados pode ser encontrado em outra máquina
    Altamente escalável
    Portatil; pode ser movido através de multiplas plataformas

Conceitos
    Blocks
        Os arquivos sao dividios em parters menores chamadas blocos
        Qtd minima de dados que pode ser lida ou escrita a prova de falhas
        O tamanho padrão dos arquivos está entre 64 e 128mb
    Nós:
        Sistema unico (maquina) responsavel por processar e armazenar os dados
        Primary node; REGULA O ACESSO aos arquivos, mantém, gerencia e atribui tarefas ao secondary node(data node)
        Secundary node; realizam solicitaçoes de leitura e gravaçõ seguindo instruçoes do primary node
        Rack awareness; escolhe os racks de nós mais próximos uns aos outros, melhora performance reduzindo trafico na rede, name node mantem o rack ID information
    Replication:
        Reconhecimento de rack para criar réplicas para garantir que os dados sejam confiáveis
        Cria uma copia do block de dados com proposito de backup
        Fator de replicação; numero de vezes que o block de dados foi copiado
    Read and Write Operations
        Write once read many

HIVE:
    É um software de data warehouse dentro do haddop capaz de ler, escrever e manipular dados do tipo tabular (datasets)
        é escalavel,
        Parecido com SQL, Hive Query Language (HiveQL) baseada no SQL

    Principais diferenças entre o Hive e um RDBMS:
        # HIVE
            Escreve uma vez le varias vezes, gerencia petabytes de dados, nao obriga o schema paara carregas dados, suporta particionamento
        # Tradicional RDBMS
            Escreve varias e le variaz vezes, gerencia megabytes, obriga o schema para carregar dados, geralmente nao suporta particionamento

    Arquitetura:
        Hive Clients
            JBDC/OBDC drivers, dependendo da aplicaçao pode ser algo baseado em java ou ODBC
        Hive Service
            Web Interface com o usuário, Hive server e linha de comando geram tarefas para o driver, que otimmiza(divide as tarefas eficientemente), planeja, executa os comandos de entrada do usuário, geralmente querys, e armazena no metastore informaçoes sobre as tabelas 
        Hive Storage and Computing
            Armazena quando solicitado pelo driver no database ou no Hadoop Cluster

HBase:
    Sistema de gerenciamento de um database nao relacional orientado a coluna, rodando sobre o HDFS, roda bem com dados em tempo real, preve uma tolerancia a falha. usado para aplicaçoes com muita gravações para armazenar grande quantidade de dados e gerar analises. Fornece um suporte de backup para as atividades do MapReduce, não tem colunas fixas sendo definido apenas por familias de colunas. Fornece uma API em java para se acessado. Fornece réplicas entre os clusters
        Tem um schema de colunas predefinido mas nao fixo, podendo ser alterado. O master node gerencia os cluster  e as regioes para executar os workers

    Diferencas entre Hbase e HDFS
        #HBase
            Armazenas dados em forma tabular
            Permite mudancas 
            Permite varias leituras e gravaçoes armazenadas no HDFS
            Permite armazenamento e processamento
        #HDFS   
            Armazena dasdos distribuidos
            arquitetura rigida
            É ajustado para escrever uma única vez e ler várias vezes
            Premiete somente armazenamento

Hands-on lab on Hadoop Map-Reduce (20 mins):

    1 - Download hadoop-3.2.3.tar.gz to your theia environment by running the following command. 
        cmd: curl https://dlcdn.apache.org/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz --output hadoop-3.2.3.tar.gz

    2- Extract the tar file in the currently directory.
        cmd: tar -xvf hadoop-3.2.3.tar.gz

    3- Navigate to the hadoop-3.2.3 directory.
        cmd: cd hadoop-3.2.3

    4- Check the hadoop command to see if it is setup. This will display the usage documentation for the hadoop script.
        cmd: bin/hadoop

    5- Run the following command to download data.txt to your current directory.
        cmd: curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/data.txt --output data.txt

    6- Run the Map reduce application for wordcount on data.txt and store the output in /user/root/output
        cmd: bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount data.txt output

    7- Once the word count runs successfully, you can run the following command to see the output file it has generated.
        cmd: ls output 

    8- Run the following command to see the word count output.
        cmd: cat  output/part-r-00000


Hands-on lab on Hadoop Map-Reduce (20 mins):

# What is a Hadoop Cluster?

    A Hadoop cluster is a collection of computers, known as nodes, that are networked together to perform parallel computations on big data sets. The Name node is  the master node of the Hadoop Distributed File System (HDFS). It maintains the meta data of the files in the RAM for quick access. An actual Hadoop Cluster  setup involves extensives resources which are not within the scope of this lab. In this lab, you will use dockerized hadoop to create a Hadoop Cluster which         will have:  

        Namenode
        Datanode
        Node Manager
        Resource manager
        Hadoop history server

    1- Clone the repository to your theia environment.
        cmd: git clone https://github.com/ibm-developer-skills-network/ooxwv-docker_hadoop.git

    2- Navigate to the docker-hadoop directory to build it.
        cmd: cd ooxwv-docker_hadoop

    3- Compose the docker application.
        cmd: docker-compose up -d
            # Compose is a tool for defining and running multi-container Docker applications. It uses the YAML file to configure the serives and enables us to create and start all the services from just one configurtation file.
    4- Run the namenode as a mounted drive on bash.
        cmd: docker exec -it namenode /bin/bash

    5- You will observe that the prompt changes as shown below.
        old: theia@theiadocker-vitorbelan:/home/project/ooxwv-docker_hadoop$ 
        new: root@a888cb13897c:/# 

    
# Explore the hadoop environment
    As you have learnt in the videos and reading thus far in the course, a Hadoop environment is configured by editing a set of configuration files:

        hadoop-env.sh:   Serves as a master file to configure YARN, HDFS, MapReduce, and Hadoop-related project settings.
        core-site.xml:   Defines HDFS and Hadoop core properties
        hdfs-site.xml:   Governs the location for storing node metadata, fsimage file and log file.
        mapred-site-xml: Lists the parameters for MapReduce configuration.
        yarn-site.xml:   Defines settings relevant to YARN. It contains configurations for the Node Manager, Resource Manager, Containers, and Application Master.
        For the docker image, these xml files have been configured already. You can see these in the directory /opt/hadoop-3.2.1/etc/hadoop/ by running
            cmd: ls /opt/hadoop-3.2.1/etc/hadoop/*.xml

    Create a file in the HDFS:
        1- In the HDFS, create a directory structure named user/root/input.
            cmd: hdfs dfs -mkdir -p /user/root/input

        2- Copy all the hadoop configuration xml files into the input directory.
            cmd: hdfs dfs -put $HADOOP_HOME/etc/hadoop/*.xml /user/root/input

        3- Create a data.txt file in the current directory.
            cmd: curl https://raw.githubusercontent.com/ibm-developer-skills-network/ooxwv-docker_hadoop/master/SampleMapReduce.txt --output data.txt 

        4- Copy the data.txt file into /user/root.
            cmd: hdfs dfs -put data.txt /user/root/

        5- Check if the file has been copied into the HDFS by viewing its content.
            cmd: hdfs dfs -cat /user/root/data.txt


